{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning",
      "Microsoft": "Warning"
    }
  },

  "AzureOpenAiChatGptDeployment": "gunvor-gpt-4o",

  "AzureDocumentIntelligenceEndpoint": "https://di-copilottest-swn.cognitiveservices.azure.com/",

  "MaxTokens": 128000,

  "PureChatGptAiAssistant": {
    "SystemPromptPath": "/Prompts/SystemPrompts/system-prompt-pure-chat-gpt.txt"
  },
  "RagChatGptAiAssistant": {
    "SystemPromptPath": "/Prompts/SystemPrompts/system-rag-chat-gpt.txt"
  },

  "IsAzureServiceHealthCheckEnabled": false,

  "AllowedHosts": "*",

  "NLog": {
    "LogLevel": {
      "Default": "Trace",
      "Microsoft": "Warning",
      "Microsoft.Hosting.Lifetime": "Information"
    }
  },

  "KernelMemory": {
    "Services": {
      "AzureAIDocIntel": {
        // "APIKey" or "AzureIdentity".
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://di-copilottest-swn.cognitiveservices.azure.com/"
      },
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://srch-copilottest-swn.search.windows.net",
        "UseHybridSearch": true
      },
      "AzureBlobs": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureBlobConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__Account' to set this
        "Account": "guncopilottestswn",
        // Container where to create directories and upload files
        "Container": "smemory",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://gunvor-copilot-test.openai.azure.com/",
        "Deployment": "gunvor-gpt-4o",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        "MaxRetries": 10
      },
      "AzureOpenAIEmbedding": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://gunvor-copilot-test.openai.azure.com/",
        "Deployment": "gunvor-ada",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191
      }
    },
    "Retrieval": {
      "SearchClient": {
        // Maximum number of tokens accepted by the LLM used to generate answers.
        // The number includes the tokens used for the answer, e.g. when using
        // GPT4-32k, set this number to 32768.
        // If the value is not set or less than one, SearchClient will use the
        // max amount of tokens supported by the model in use.
        "MaxAskPromptSize": -1,
        // Maximum number of relevant sources to consider when generating an answer.
        // The value is also used as the max number of results returned by SearchAsync
        // when passing a limit less or equal to zero.
        "MaxMatchesCount": 10,
        // How many tokens to reserve for the answer generated by the LLM.
        // E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
        // the prompt sent to LLM will contain max 3700 tokens, composed by
        // prompt + question + grounding information retrieved from memory.
        "AnswerTokens": 300,
        // Text to return when the LLM cannot produce an answer.
        "EmptyAnswer": "INFO NOT FOUND"
      }
    },

    "AppSettings": {
      "BackendUri": "https://localhost:7181",
      "FrontendUri": "http://localhost:4206"
    }
  }
}
