{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft": "Warning",
      "Microsoft.Hosting.Lifetime": "Information"
    }
  },
  "NLog": {
    "autoReload": true,
    "throwConfigExceptions": true,
    "targets": {
      "file": {
        "type": "File",
        "fileName": "logs/logfile.log",
        "layout": "${longdate}|${level:uppercase=true}|${logger}|${message}${exception:format=ToString}"
      },
      "console": {
        "type": "Console"
      }
    },
    "rules": [
      {
        "logger": "*",
        "minLevel": "Info",
        "writeTo": "console"
      },
      {
        "logger": "*",
        "minLevel": "Debug",
        "writeTo": "file"
      }
    ]
  },

  "AzureOpenAiChatGptDeployment": "hack-gpt4o",

  "AzureDocumentIntelligenceEndpoint": "https://di-copilottest-swn.cognitiveservices.azure.com/",

  "MaxTokens": 128000,

  "RagChatGptAiAssistant": {
    "SystemPromptPath": "/Prompts/system-rag-chat-gpt.txt"
  },

  "AllowedHosts": "*",

  "AzureStorageAccountEndpoint": "https://hackragstorage.blob.core.windows.net",
  "AzureStorageContainer": "test",

  "KernelMemory": {
    "Services": {
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://hack-rag-search.search.windows.net",
        "UseHybridSearch": true
      },
      "AzureBlobs": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureBlobConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__Account' to set this
        "Account": "hackragstorage",
        // Container where to create directories and upload files
        "Container": "memory",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://hack-rag-openai.openai.azure.com/",
        "Deployment": "hack-gpt4o",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        "MaxRetries": 10
      },
      "AzureOpenAIEmbedding": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://hack-rag-openai.openai.azure.com/",
        "Deployment": "hack-ada",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191
      }
    },
    "Retrieval": {
      "SearchClient": {
        // Maximum number of tokens accepted by the LLM used to generate answers.
        // The number includes the tokens used for the answer, e.g. when using
        // GPT4-32k, set this number to 32768.
        // If the value is not set or less than one, SearchClient will use the
        // max amount of tokens supported by the model in use.
        "MaxAskPromptSize": -1,
        // Maximum number of relevant sources to consider when generating an answer.
        // The value is also used as the max number of results returned by SearchAsync
        // when passing a limit less or equal to zero.
        "MaxMatchesCount": 10,
        // How many tokens to reserve for the answer generated by the LLM.
        // E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
        // the prompt sent to LLM will contain max 3700 tokens, composed by
        // prompt + question + grounding information retrieved from memory.
        "AnswerTokens": 300,
        // Text to return when the LLM cannot produce an answer.
        "EmptyAnswer": "INFO NOT FOUND"
      }
    },

    "AppSettings": {
      "BackendUri": "https://localhost:7181",
      "FrontendUri": "http://localhost:4206"
    }
  }
}
